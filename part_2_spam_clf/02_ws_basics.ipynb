{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: How to train a classifier using Weak Supervision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to train a spam detection classifier using weakly supervised data. \n",
    "\n",
    "The steps:\n",
    "- Collect training data\n",
    "- Annotate this data in a weakly supervised setting\n",
    "    - Create labeling functions\n",
    "    - *Match* the labeling functions to the data samples\n",
    "    - Aggregate the labels with different label aggregation techniques\n",
    "        - Majority Vote\n",
    "        - FABLE \n",
    "- train a logistic regression classifier using weak labels\n",
    "- train a logistic regresison classifier with SepLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "from wrench.utils import set_seed\n",
    "from wrench.endmodel import EndClassifierModel\n",
    "from wrench._logging import LoggingHandler\n",
    "\n",
    "\n",
    "from snorkel.utils import probs_to_preds\n",
    "from utils import load_raw_spam_dataset\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the folder where our data is stored\n",
    "\n",
    "path_to_data = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T13:52:02.389214Z",
     "start_time": "2023-08-10T13:52:02.374441Z"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use for training is Spam Detection YouTube comments dataset \n",
    "[3]. \n",
    "\n",
    "- The dataset consists of comments that YouTube users left under different videos.\n",
    "- Each sample is a comment (i.e., a word, a sentence, or a couple of sentences).\n",
    "- 1,586 train samples, 120 dev samples, 250 test samples\n",
    "- There are 2 types of samples:\n",
    "    - HAM: comments relevant to the video (even very simple ones), or\n",
    "    - SPAM: irrelevant (often trying to advertise something) or inappropriate messages\n",
    "    \n",
    "<img src=\"../img/spam_detection.png\" width=\"800\"/>\n",
    "\n",
    "**NB! Original dataset is manually labeled, but we won't use these gold labels for model training! We will use the dataset as unlabeled one (and label it in a weakly-supervised fasion).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YouTube dataset\n",
    "\n",
    "df_train, df_dev, df_test = load_raw_spam_dataset(load_train_labels=True)\n",
    "# Y_train = df_train[\"label\"].values\n",
    "# Y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data sample in the original dataset (i.e., a YouTube comment), we know:\n",
    "- comment's author,\n",
    "- date when the corresponding comment was left,\n",
    "- text of the sample,\n",
    "- gold manual label,\n",
    "- id of the YouTube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples of positive (=non-spam) samples, label id 0\n",
    "\n",
    "df_train.loc[df_train[\"label\"]==0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examples of negative (=spam) samples, label id 0\n",
    "\n",
    "df_train.loc[df_train[\"label\"]==1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[\"text\", \"label\"]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to start weak supervision! So, let's imagin the gold labels disappeared... \n",
    "\n",
    "<img src=\"../img/poof.jpg\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and here we are: there is some data we want to use for classifier training, but we don't have any labels and capacity/time/money/... for hiring annotators.\n",
    "\n",
    "But we can label this data with **weak supervision** :)\n",
    "\n",
    "<img src=\"../img/rainbow.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Supervision\n",
    "\n",
    "A brief reminder how weak supervision works:\n",
    "1. We come up with some heuristic rules and transform these rules into labeling functions.\n",
    "2. We apply these labeling functions to the data and obtain weak labels.\n",
    "3. We use this weak labels to train a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the training samples we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.text[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T21:04:15.013913Z",
     "start_time": "2023-08-09T21:04:15.007312Z"
    }
   },
   "source": [
    "## Task: formulate the rules that could annotate the training samples\n",
    "\n",
    "The questions that might help you: \n",
    "\n",
    "*What patterns are typical for spam YouTube comments? for non-spam comments?*\n",
    "\n",
    "*What rules might help to distinguish between spam and not-spam YouTube comments?*\n",
    "\n",
    "*What labeling functions do you think are **productive** and **useful** to annotate the YouTube comments?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules: \n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ...\n",
    "6. ...\n",
    "7. ...\n",
    "8. ...\n",
    "9. ...\n",
    "10. ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My examples of rules: \n",
    "- \"check\"/\"check out\": if there is a collocation \"check out\" in the comment, most probably this comment is spam (and the comment author is promoting his/her channel)\n",
    "- \"subscribe\": same\n",
    "- \"my\": same\n",
    "- ...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can be a rule?\n",
    "\n",
    "- Keyword searches: looking for specific words in a sentence\n",
    "- Pattern matching: looking for specific syntactical patterns\n",
    "- Third-party models: using an pre-trained model (usually a model for a different task than the one at hand)\n",
    "- ...\n",
    "- Crowdworker labels: treating each crowdworker as a black-box function that assigns labels to subsets of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules into labeling functions\n",
    "\n",
    "After we collected some rules, we transform them into labeling functions that could *label* the data sample - that is, assign it to one or another class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of LF based on a keyword \"check out\"\n",
    "\n",
    "def check_out(x):\n",
    "    return 1 if \"check out\" in x.text.lower() else -1\n",
    "\n",
    "# meaning the sample will be assigned to class 1 (=SPAM) if there is a \"check out\" expression in the comment, \n",
    "# otherwise to class 0 (=non-SPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of LF based on a key word \"please\"\n",
    "\n",
    "def check(x):\n",
    "    return 1 if \"please\" in x.text.lower() else -1\n",
    "\n",
    "# meaning the sample will be assigned to class 1 (=SPAM) if there is a \"please\" expression in the comment, \n",
    "# otherwise to class 0 (=non-SPAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T22:01:27.414881Z",
     "start_time": "2023-08-09T22:01:26.383285Z"
    }
   },
   "source": [
    "### Labeling functions we are going to use\n",
    "\n",
    "In this tutorial, we are going to use the labeling functions created by [Snorkel team](https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/01_spam_tutorial.ipynb), which are: \n",
    "\n",
    "\n",
    "1. keyword **\"my\"** (to detect spam comments like \"my channel\", \"my video\", etc)\n",
    "2. keyword **\"subscribe\"** (to detect spam comments that ask users to subscribe to some channel)\n",
    "3. keyword **\"http\"** (to detect spam comments that link to other channels)\n",
    "4. keyword **\"please\"/\"plz\"** (to detect spam comments that make requests rather than commenting)\n",
    "5. keyword **\"song\"** (to detect non-spam comments that actually talk about the video's content)\n",
    "6. regex **\"check_out\"** (to detect spam comments like \"check out this channel\", etc)\n",
    "7. **short comment** (non-spam comments are often short, such as 'cool video!')\n",
    "8. **mentioning specific people** and are **short** (using SpaCy library; non-spam comments usually mention some people)\n",
    "9. **polarity** (using TextBlob library; if polarity > 0.9, it is most probably a non-spam message)\n",
    "10. **subjectivity** (using TextBlob library; if subjectivity >= 0.5, it is most probably a non-spam message)\n",
    "\n",
    "(We are not going into details of the labeling process here now - you will hear more about it from my colleagues later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed data\n",
    "\n",
    "The resulted annotations can be saved in the following format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/youtube/train.json\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "train_data[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T14:38:12.461055Z",
     "start_time": "2023-08-10T14:38:12.445819Z"
    }
   },
   "source": [
    "The structure of the processed data is the following: \n",
    "- data.text: the text of the sample\n",
    "- label: gold label obtained by manual annotation\n",
    "- weak_labels: the results of annotation by labeling functions. \n",
    "    - -1: the corresponding labeling function did not match\n",
    "    - 0: the labeling function matched and assigned this sample to class 0 (non-spam class in our case)\n",
    "    - 1: the labeling function matched and assigned this sample to class 1 (spam class in our case)\n",
    "\n",
    "So, for the sample #1:\n",
    "(*if your like drones, plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful.\\ufeff*)\n",
    "\n",
    "- labeling functions 1, 3, 5, 6, 7, 8, 9 did not match\n",
    "- labeling functions 2 (a key word *subscribe*) & 4 (a key word *plz*) matched and assigned this sample to the class 1\n",
    "- labeling function 10 (subjectivity score > 0.5) matched and assigned this sample to the class 0\n",
    "\n",
    "**Next step: how to turn these annotations into weak labels to train a classifier with them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T15:48:13.475728Z",
     "start_time": "2023-08-10T15:48:13.094259Z"
    }
   },
   "source": [
    "## Weak labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-10T15:48:15.062358Z",
     "start_time": "2023-08-10T15:48:15.049170Z"
    }
   },
   "source": [
    "There are different *label models* that calculate the weak labels based on labeling functions annotations. In this tutorial, we are going to try two of them: \n",
    "\n",
    "- **Majority Vote** (intuitive and straightforward)\n",
    "- **FABLE** [1] (most recent and well-performing)\n",
    "\n",
    "For label calculation and model training we will use a weakly supervised framework called [Wrench](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRmYabjOGAAxW1h_0HHQt3COQQFnoECA4QAQ&url=https%3A%2F%2Fgithub.com%2FJieyuZ2%2Fwrench&usg=AOvVaw3EWVM0icLVHENbUv51USa_&opi=89978449) [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrench dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we transform our data into a Wrench-specific dataset.\n",
    "\n",
    "We can encode the data with TF-IDF features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features\n",
    "\n",
    "from wrench.dataset import load_dataset\n",
    "\n",
    "train_data_tfidf, valid_data_tfidf, test_data_tfidf = load_dataset(\n",
    "    path_to_data,     # path to the folder where the dataset is stored\n",
    "    \"youtube\",         # name of the dataset\n",
    "    extract_feature=True,      # we want to encode out data ...\n",
    "    extract_fn='tfidf'        # ... with TF-IDF features (other predefined options are 'sentence_transformer', 'bert')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or with BERT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert features\n",
    "\n",
    "train_data, valid_data, test_data = load_dataset(\n",
    "    path_to_data,       # path to the folder where the dataset is stored\n",
    "    \"youtube\",    # name of the dataset\n",
    "    extract_feature=True,      # we want to encode out data ...\n",
    "    extract_fn='bert',        # ... with bert embeddings\n",
    "    model_name='bert-base-cased',      # the name of the bert model\n",
    "    cache_name='bert'     # load it from cache if there are cached files \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what's inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the format of the train_data, valida_data, and test_data now is: wrench.dataset.dataset.TextDataset\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many classes are there in the dataset?\n",
    "\n",
    "train_data.n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many labeling functions are there in the dataset?\n",
    "\n",
    "train_data.n_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the class_id to class correspondence?\n",
    "\n",
    "train_data.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the samples look like?\n",
    "\n",
    "train_data.examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do the encoded samples look like?\n",
    "\n",
    "print(type(train_data.features))\n",
    "train_data.features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the weak annotations produced by labeling functions?\n",
    "\n",
    "train_data.weak_labels[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T21:57:30.953595Z",
     "start_time": "2023-08-09T21:57:30.941063Z"
    }
   },
   "source": [
    "### Majority Vote\n",
    "\n",
    "The simplest and most straightforward method to calculate labels from the noisy annotations is **majority voting** - a decision-making method where the option with the most votes is chosen. It's like asking a group of people to pick a movie, and the one that gets the most hands raised wins. \n",
    "\n",
    "In our case, each labeling function produces a *vote*; the most voted class is selected as a sample label. All ties are broken randomly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:  write your own majority vote function\n",
    "- Input: the weak annotations produced by labeling functions (stored in weak_labels field of wrench dataset objects)\n",
    "- Output: labels\n",
    "\n",
    "Before you start programming, think about possible bottlenecks: \n",
    "- what if a sample obtains equal amount of votes for some class?\n",
    "- what if there are no votes for a sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "def majority_vote(weak_annotations):\n",
    "    # calculate labels with majority vote \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mv = majority_vote(train_data.weak_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ready solution to aggregate the weak labels with majority vote is already included to the Wrench framework:`MajorityVoting` label model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the majority vote label model from the Wrench framework\n",
    "\n",
    "from wrench.labelmodel import MajorityVoting\n",
    "\n",
    "label_model = MajorityVoting()\n",
    "label_model.fit(dataset_train=train_data, dataset_valid=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weak labels \n",
    "\n",
    "soft_label_mv = label_model.predict_proba(train_data)    # soft label as probabilities across all classes\n",
    "hard_label_mv = probs_to_preds(soft_label_mv)               # hard labels as the most probable classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sentences, their weak annotations, and the weak labels obtained with majority voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.weak_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label_mv[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_label_mv[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FABLE \n",
    "\n",
    "Fable [1] is a label model where noisy labels are inferred not only based on the labeling functions' votes, but also using the instance features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and apply the fable model\n",
    "\n",
    "from wrench.labelmodel import Fable\n",
    "\n",
    "label_model = Fable(kernel_function=None, num_groups=3)\n",
    "_ = label_model.fit(dataset_train=train_data, dataset_valid=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate labels\n",
    "soft_label_fable = label_model.predict_proba(train_data)\n",
    "hard_label_fable = probs_to_preds(soft_label_fable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label_fable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_label_fable[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier with majorty vote hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# initialize a classifier\n",
    "model = EndClassifierModel(\n",
    "    batch_size=batch_size, test_batch_size=test_batch_size\n",
    ")\n",
    "\n",
    "# fit it on the training data + majority vote hard labels\n",
    "model.fit(\n",
    "    dataset_train=train_data, \n",
    "    y_train=hard_label_mv, \n",
    "    dataset_valid=valid_data, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# test on the test set\n",
    "model.test(dataset=test_data, metric_fn=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier with FABLE hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# initialize a classifier\n",
    "model = EndClassifierModel(\n",
    "    batch_size=batch_size, test_batch_size=test_batch_size\n",
    ")\n",
    "\n",
    "# fit it \n",
    "model.fit(\n",
    "    dataset_train=train_data, \n",
    "    y_train=hard_label_fable, \n",
    "    dataset_valid=valid_data,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# test on the test set\n",
    "model.test(dataset=test_data, metric_fn=\"acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-2-End training with SepLL\n",
    "\n",
    "In the following, we use a state-of-the-art method called SepLL [4] to train a classifier with weak labels. During training, LF matches are the only training signal, and prediction is then later made from a latent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrench.classification.sepll import SepLL\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "bert_model_name = 'roberta-base'\n",
    "\n",
    "#### Initialize SepLL\n",
    "model = SepLL(\n",
    "    batch_size=batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    backbone='MLP',\n",
    "    backbone_model_name=bert_model_name,\n",
    "    # \n",
    "    # SepLL specific\n",
    "    add_unlabeled=False,\n",
    "    class_noise=0.0,\n",
    "    lf_l2_regularization=0.05,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    dataset_train=train_data,\n",
    "    dataset_valid=valid_data,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "acc = model.test(test_data, 'acc')\n",
    "\n",
    "logger.info(f'SepLL test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GPU training\n",
    "\n",
    "In case your environment has a GPU available, it is also possible to make use of the full strength of SepLL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wrench.classification.sepll import SepLL\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "batch_size=16\n",
    "bert_model_name = 'roberta-base'\n",
    "\n",
    "#### Initialize SepLL\n",
    "model = SepLL(\n",
    "    batch_size=batch_size,\n",
    "    real_batch_size=batch_size,\n",
    "    test_batch_size=test_batch_size,\n",
    "    # BERT specific parameters\n",
    "    backbone='BERT',\n",
    "    backbone_model_name=bert_model_name,\n",
    "    optimizer='Adam',\n",
    "    optimizer_lr=5e-5,\n",
    "    optimizer_weight_decay=0.0,\n",
    "    \n",
    "    # SepLL specific\n",
    "    add_unlabeled=False,\n",
    "    class_noise=0.0,\n",
    "    lf_l2_regularization=0.5,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    dataset_train=train_data,\n",
    "    dataset_valid=valid_data,\n",
    "    metric='acc',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = model.test(test_data, 'acc')\n",
    "\n",
    "logger.info(f'SepLL test acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Zhang et al. 2023. Leveraging Instance Features for Label Aggregation in Programmatic Weak Supervision. https://arxiv.org/abs/2210.02724 \n",
    "2. Zhang et al. 2021 WRENCH: A Comprehensive Benchmark for Weak Supervision. https://arxiv.org/abs/2109.11377\n",
    "3. Alberto TC et al.  2015. Tubespam: Comment Spam Filtering on Youtube. https://ieeexplore.ieee.org/document/7424299\n",
    "4. Stephan et al. 2022. SepLL: Separating Latent Class Labels from Weak Supervision Noise. https://arxiv.org/abs/2210.13898\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
